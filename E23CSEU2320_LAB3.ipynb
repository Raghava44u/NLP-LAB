{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58fa0f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing\n"
     ]
    }
   ],
   "source": [
    "print(\"testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "775dc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.tokenize\n",
    "import nltk.corpus\n",
    "from nltk.stem import PorterStemmer , WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "132b5ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: wrapt in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nares\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "   ---------------------------------------- 0.0/24.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/24.0 MB 5.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 2.1/24.0 MB 5.1 MB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 3.1/24.0 MB 4.9 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 4.2/24.0 MB 5.0 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 5.2/24.0 MB 5.0 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 6.3/24.0 MB 5.1 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 7.6/24.0 MB 5.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 8.7/24.0 MB 5.3 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 10.5/24.0 MB 5.6 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 12.8/24.0 MB 6.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 13.9/24.0 MB 6.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 15.5/24.0 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 16.8/24.0 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 18.4/24.0 MB 6.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.2/24.0 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 22.3/24.0 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  23.9/24.0 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.0/24.0 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.6/46.2 MB 7.0 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 2.6/46.2 MB 6.9 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 3.9/46.2 MB 6.2 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 5.0/46.2 MB 5.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 6.0/46.2 MB 5.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 7.3/46.2 MB 5.8 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 8.7/46.2 MB 6.0 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 10.5/46.2 MB 6.2 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 11.8/46.2 MB 6.3 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 13.1/46.2 MB 6.4 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 13.9/46.2 MB 6.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 14.9/46.2 MB 6.1 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 15.5/46.2 MB 6.0 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 17.0/46.2 MB 5.9 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 18.6/46.2 MB 6.1 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 20.4/46.2 MB 6.2 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 22.3/46.2 MB 6.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 24.6/46.2 MB 6.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 27.0/46.2 MB 6.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 28.8/46.2 MB 7.0 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 31.2/46.2 MB 7.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 32.8/46.2 MB 7.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 35.1/46.2 MB 7.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 37.5/46.2 MB 7.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 40.1/46.2 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.5/46.2 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 8.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 8.1 MB/s eta 0:00:00\n",
      "Installing collected packages: scipy, gensim\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.15.1\n",
      "    Uninstalling scipy-1.15.1:\n",
      "      Successfully uninstalled scipy-1.15.1\n",
      "Successfully installed gensim-4.3.3 scipy-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9d461a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd00af41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nares\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nares\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4728acdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Education is the most powerful weapon.,',\n",
       " 'Students go to school to learn new things.,',\n",
       " 'Teachers provide knowledge and guidance.,',\n",
       " 'Books are essential for gaining education.,',\n",
       " 'A school is a place where learning happens.,',\n",
       " 'Classrooms are filled with enthusiastic learners.,',\n",
       " 'Learning builds a strong foundation.,',\n",
       " 'Good education shapes the future of society.,',\n",
       " 'Exams test the knowledge of students.,',\n",
       " 'Children must be encouraged to go to school.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the data from text2.txt file from folder.\n",
    "def read_text_file():\n",
    "    with open('text2.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    return lines\n",
    "read_text_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49630beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess and tokenize text.\n",
    "def preprocess_text(lines):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        words = word_tokenize(line.lower())\n",
    "        words = [word for word in words if word not in string.punctuation and word not in stop_words]\n",
    "        processed_lines.append(words)\n",
    "    return processed_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65328f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec(cbow) model\n",
    "def train_word2vec(tokenized_sentences, vector_size=100, window=5, min_count=1, sg=0):\n",
    "    model = Word2Vec(sentences=tokenized_sentences, vector_size=vector_size,\n",
    "                     window=window, min_count=min_count, sg=sg)\n",
    "    return model\n",
    "\n",
    "# topn 5 similar words\n",
    "def find_similar_words(model, word, topn=5):\n",
    "    if word in model.wv:\n",
    "        return model.wv.most_similar(word, topn=topn)\n",
    "    else:\n",
    "        return f\"'{word}' not in vocabulary\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88e2312b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 words 'education':\n",
      "new: 0.2529\n",
      "classrooms: 0.2008\n",
      "builds: 0.1753\n",
      "knowledge: 0.1701\n",
      "learn: 0.1502\n"
     ]
    }
   ],
   "source": [
    "sentences = read_text_file()\n",
    "tokenized_sentences = preprocess_text(sentences)\n",
    "w2v_model = train_word2vec(tokenized_sentences, sg=0)\n",
    "similar_words = find_similar_words(w2v_model, 'education', topn=5)\n",
    "print(\"top 5 words 'education':\")\n",
    "for word, score in similar_words:\n",
    "    print(f\"{word}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb0d5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2. Apply TF-IDF using TfidfVectorizer on 5–10 English sentences.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def preprocess_text(lines):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    processed_lines = []\n",
    "    for line in lines:\n",
    "        words = word_tokenize(line.lower())\n",
    "        words = [word for word in words if word not in string.punctuation and word not in stop_words]\n",
    "        processed_lines.append(words)\n",
    "    return processed_lines\n",
    "\n",
    "processed_sentences = preprocess_text(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "20b1d1ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['education', 'powerful', 'weapon.'],\n",
       " ['students', 'go', 'school', 'learn', 'new', 'things.'],\n",
       " ['teachers', 'provide', 'knowledge', 'guidance.'],\n",
       " ['books', 'essential', 'gaining', 'education.'],\n",
       " ['school', 'place', 'learning', 'happens.'],\n",
       " ['classrooms', 'filled', 'enthusiastic', 'learners.'],\n",
       " ['learning', 'builds', 'strong', 'foundation.'],\n",
       " ['good', 'education', 'shapes', 'future', 'society.'],\n",
       " ['exams', 'test', 'knowledge', 'students.'],\n",
       " ['children', 'must', 'encouraged', 'go', 'school']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f04b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names:\n",
      "['attend' 'becoming' 'better' 'education' 'ensures' 'future' 'gain' 'in'\n",
      " 'is' 'key' 'knowledge' 'learning' 'life' 'many' 'more' 'online'\n",
      " 'opportunities' 'play' 'popular' 'quality' 'role' 'school' 'students'\n",
      " 'success' 'teachers' 'the' 'to' 'vital']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.         0.         0.         0.26470068 0.         0.\n",
      "  0.         0.31888178 0.31888178 0.39524574 0.         0.\n",
      "  0.39524574 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.39524574\n",
      "  0.         0.39524574 0.31888178 0.        ]\n",
      " [0.39835162 0.         0.         0.         0.         0.\n",
      "  0.39835162 0.         0.         0.         0.39835162 0.\n",
      "  0.         0.39835162 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.39835162 0.32138758 0.\n",
      "  0.         0.         0.32138758 0.        ]\n",
      " [0.         0.4428322  0.         0.29656989 0.         0.\n",
      "  0.         0.         0.35727423 0.         0.         0.\n",
      "  0.         0.         0.4428322  0.4428322  0.         0.\n",
      "  0.4428322  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.32138758 0.         0.         0.         0.39835162\n",
      "  0.         0.         0.         0.         0.         0.39835162\n",
      "  0.         0.         0.39835162 0.         0.32138758 0.\n",
      "  0.39835162 0.         0.         0.39835162]\n",
      " [0.         0.         0.42841136 0.28691208 0.42841136 0.42841136\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.42841136 0.\n",
      "  0.         0.42841136 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Q2. Apply TF-IDF using TfidfVectorizer on 5 English sentences.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string\n",
    "\n",
    "sentences = [\n",
    "    \"Education is the key to success in life.\",\n",
    "    \"Many students attend school to gain knowledge.\",\n",
    "    \"Online education is becoming more popular.\",\n",
    "    \"Teachers play a vital role in a student's learning.\",\n",
    "    \"Quality education ensures better future opportunities.\"\n",
    "]\n",
    "\n",
    "processed_sentences = [\n",
    "    s.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    for s in sentences\n",
    "]\n",
    "\n",
    "# TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_sentences)\n",
    "\n",
    "# Display\n",
    "print(\"Feature Names:\")\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12b8e19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between the sentences: 0.1274\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "sentence1 = \"Students study in school.\"\n",
    "sentence2 = \"Students learn at educational institutes.\"\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "sentences = [preprocess(sentence1), preprocess(sentence2)]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "print(f\"Cosine similarity between the sentences: {cos_sim[0][0]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b5e63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
